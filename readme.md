# Machine Learning
## C1 KNN(k-nearest neighbor)
> 个人感觉在运行时每次检测一个test目标时都要k个投票时间复杂度有点过于高了
### simple_knn
- 简单二维分类
- 使用欧氏距离计算test与数据集差值，取k个最小距离依次按类别投票
### 海伦约会
- 简单三维分类
- 同样使用欧氏距离，使用相同权值，归一化到0-1区间
- 因为不喜欢命令行运行，将所有路径改为相对路径
### 手写数字识别
- 原始输入32*32 更改输入维度（1，1024）
- label为每个txt文件的名字，203个训练集，88个验证集（个人认为比例不妥）
- 错误率大概在0.0125%左右
- 不如我在Mnist上的成果：1、训练量少 2、训练模型方式较简单 3、数据集分配比例
- 因为不喜欢命令行运行，将所有路径改为相对路径
---
## C2  Decision Tree 决策树
> 感觉代码构建复杂得要死，适合简单的区分,大点的数据加多点的种类光构建模型就够麻烦了，不过构建完运行时的效率应该比KNN高
### 构建结构
> 个人对构建理解为：信息熵增益效果即为权重的选择，越重要的放得越靠近根节点，增益值越小影响越大，影响最大的必然放在根节点
- 决策树训练使用流程：
- 基本不需要数据预处理，只需要对种类数量的判断是否足够训练、种类是否重合等简单情况要处理
- 先寻找每类分割阈值，子集尽可能纯（ID3、Gini、C4.5等）
> **ID3**  
> $$  
> Gain(D, A) = Entropy(D) - \sum_{v \in values(A)} \frac{|D_v|}{|D|} Entropy(D_v)  
> $$  
> 选择信息增益最大的划分属性,信息增益（Gain）越大，说明分裂后带来的“纯度提升”越多，就越应该放到根节点
> **Gini (CART)**  
> $$  
> Gini(D) = 1 - \sum_{i=1}^{k} p_i^2  
> $$  
> 为 0 时最纯,Gini 越低越好
> **C4.5**  
> $$  
> GainRatio(D, A) = \frac{Gain(D, A)}{IV(A)}  
> $$  
> 用于修正信息增益对多值属性的偏好,信息增益率（GainRatio）越大也越优先

- **预剪枝（限制树深或最小样本数）和后剪枝（用验证集剪枝），防止过拟合。** *暂时没学到*
- 之后递归构建决策树，新样本在根节点按阈值向左或右走，直到叶节点给出预测。
ID3：容易偏好多值特征；
C4.5：解决了这一问题但计算较复杂；
CART：同时支持分类和回归，计算更高效。
- 使用时根据阈值来走即可
---
## C3 朴素贝叶斯
> 个人理解：通过加入其他条件对先验概率的影响，判断影响结果的后验概率
- **拉普拉斯平滑**解决概率为零导致结果乘积为零的情况
- **对数处理结果**解决下溢出的问题
*计算机小数乘小数越来越小，四舍五入最终为0*

- bayes-modify.py 里英文判断 用split创建文库训练
- nbc.py 里中文判断 用**jieba**创建文库训练
- nbc.py 需要去除高频词
**bayes process**:
- 首先预处理，如文字分割，摒弃多个样本多次出现的文字符号
- 其次朴素贝叶斯算法：
```
MultinomialNB
主要用于离散型变量
多用于文本分类
```
```
GaussianNB
主要用于连续型变量
```
```
GaussianNB
用于0 1 bool型变量
判断是否
```
- 最后选择最大后验概率作为结果输出
- 先验后验相乘，哪个高即为哪种情况
---
## C4 Logistic回归
> 个人感觉有点像激活函数的东西，但是这玩意梯度上升
- 同样设置有学习率（步长），通过迭代找到极大值
- 利用求出的值做test，一般与0.5作比较作区分
- 主要就是一个线性回归
> 感觉是线性SVM的基础前身
---
## C5 SVM
> 学之前觉得是比较落后的算法，但是升维的想法极其nb
- **线性**：
就是在超平面内找到距离两个类别距离最远的分割线
- **非线性**：
- 用核函数把低维映射到高维
> 感觉SVM能完美解决一切分类问题，一个不行就多来几个，多升几个维度，一定能完美解出来（不考虑能耗代价）
数学基础不太够先放着，转战jupyter吴恩达
- 主要流程
- 先设置基本函数
- 寻找最佳的向量
- test时相乘或做内积，通过权重向量方向得出正负，即可二分类
---
## C6 Adaboost
> 之前没听过，感觉像是一个合并器或者说是分化器，像是在做预处理
- bagging & boosting
前者类似并行的训练方式，时间复杂度会降低
后者分步训练，效果质量应该会比相同步数的前者高
- *Bagging*：将数据集分成k份，用相同权重利用相同算法去训练
- *Boosting*：将分类结果作为每步分类目标，我认为是每步都在做类似二分类的分类




# conclusionfor W2D3
**总的来说感觉ML是在打基础，全是算法，我认为这是在为不同数据处理怎么处理怎么选择算法优化的一条路**
- 代码流程：基本了解框架与每行代码作用
- 算法优劣势基本有了解
**KNN用来二分类 小数据 时空间较复杂** 在某一维度选某一距离计算可能能做到多分类，但是难
*但是不清楚为什么能这么分 数据多了杂了估计准确度会有下降*
**DT用来多分类 中等数据 构造时麻烦** 
*不知道有什么突出优势*
*贝叶斯 稍作了解 是将各个类别之间做关联的抽象解释 达到区分概念*

# conclusion for W2D4
**NB 三种模型，一种用来二分类，另外两种可以做到多分类，样本类型不同**
**logistic SVM的基础，简单的回归**
**SVM的思路很棒，适合也只适合做二分类，功能比较nb，若想高质量分类训练运行需求比较高**