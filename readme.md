# Machine Learning
## C1 KNN代码练习
> 个人感觉在运行时每次检测一个test目标时都要k个投票时间复杂度有点过于高了
### simple_knn
- 简单二维分类
- 使用欧氏距离计算test与数据集差值，取k个最小距离依次按类别投票
### 海伦约会
- 简单三维分类
- 同样使用欧氏距离，使用相同权值，归一化到0-1区间
- 因为不喜欢命令行运行，将所有路径改为相对路径
### 手写数字识别
- 原始输入32*32 更改输入维度（1，1024）
- label为每个txt文件的名字，203个训练集，88个验证集（个人认为比例不妥）
- 错误率大概在0.0125%左右
- 不如我在Mnist上的成果：1、训练量少 2、训练模型方式较简单 3、数据集分配比例
- 因为不喜欢命令行运行，将所有路径改为相对路径

## C2  Decision Tree 决策树代码练习
> 感觉代码构建复杂得要死，适合简单的区分,大点的数据加多点的种类光构建模型就够麻烦了，不过构建完运行时的效率应该比KNN高
### 构建结构
> 个人对构建理解为：信息熵增益效果即为权重的选择，越重要的放得越靠近根节点，增益值越小影响越大，影响最大的必然放在根节点
- 决策树训练使用流程：
- 基本不需要数据预处理，只需要对种类数量的判断是否足够训练、种类是否重合等简单情况要处理
- 先寻找每类分割阈值，子集尽可能纯（ID3、Gini、C4.5等）
> **ID3**  
> $$  
> Gain(D, A) = Entropy(D) - \sum_{v \in values(A)} \frac{|D_v|}{|D|} Entropy(D_v)  
> $$  
> 选择信息增益最大的划分属性,信息增益（Gain）越大，说明分裂后带来的“纯度提升”越多，就越应该放到根节点

---

> **Gini (CART)**  
> $$  
> Gini(D) = 1 - \sum_{i=1}^{k} p_i^2  
> $$  
> 为 0 时最纯,Gini 越低越好

---

> **C4.5**  
> $$  
> GainRatio(D, A) = \frac{Gain(D, A)}{IV(A)}  
> $$  
> 用于修正信息增益对多值属性的偏好,信息增益率（GainRatio）越大也越优先

- 预剪枝（限制树深或最小样本数）和后剪枝（用验证集剪枝），防止过拟合。
- 之后递归构建决策树，新样本在根节点按阈值向左或右走，直到叶节点给出预测。
ID3：容易偏好多值特征；
C4.5：解决了这一问题但计算较复杂；
CART：同时支持分类和回归，计算更高效。
- 使用时根据阈值来走即可