<!-- 启用 MathJax（仅在支持 HTML 的环境中有效，如 GitHub Pages、Jekyll、本地 Markdown 预览插件） -->
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
# Machine Learning
## knn代码练习
因为不喜欢命令行运行，将所有路径改为相对路径
### simple_knn
- 简单二维分类
- 使用欧氏距离计算test与数据集差值，取k个最小距离依次按类别投票
### 海伦约会
- 简单三维分类
- 同样使用欧氏距离，使用相同权值，归一化到0-1区间
### 手写数字识别
- 原始输入32*32 更改输入维度（1，1024）
- label为每个txt文件的名字，203个训练集，88个验证集（个人认为比例不妥）
- 错误率大概在0.0125%左右
- 不如我在Mnist上的成果：1、训练量少 2、训练模型方式较简单 3、数据集分配比例

## Decision Tree 决策树代码练习
感觉代码构建复杂得要死，适合简单的区分。
### 构建结构
> 个人对构建理解为：信息熵增益效果即为权重的选择，越重要的放得越靠近根节点，增益值越小影响越大，影响最大的必然放在根节点
- 决策树训练使用流程：
- 基本不需要数据预处理，只需要对种类数量的判断是否足够训练、种类是否重合等简单情况要处理
- 先寻找每类分割阈值，子集尽可能纯（ID3、Gini、C4.5等）
> **ID3**  
> $$  
> Gain(D, A) = Entropy(D) - \sum_{v \in values(A)} \frac{|D_v|}{|D|} Entropy(D_v)  
> $$  
> 选择信息增益最大的划分属性

---

> **Gini (CART)**  
> $$  
> Gini(D) = 1 - \sum_{i=1}^{k} p_i^2  
> $$  
> 为 0 时最纯

---

> **C4.5**  
> $$  
> GainRatio(D, A) = \frac{Gain(D, A)}{IV(A)}  
> $$  
> 用于修正信息增益对多值属性的偏好


- 之后递归构建决策树，满足在左，不满足在右之类
- 使用时根据阈值来走即可